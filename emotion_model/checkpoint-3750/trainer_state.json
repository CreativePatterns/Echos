{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 3750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008,
      "grad_norm": 1.0198479890823364,
      "learning_rate": 4.9880000000000004e-05,
      "loss": 0.5669,
      "step": 10
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.9523846507072449,
      "learning_rate": 4.974666666666667e-05,
      "loss": 0.3427,
      "step": 20
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.6991226077079773,
      "learning_rate": 4.9613333333333335e-05,
      "loss": 0.2455,
      "step": 30
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.5637122988700867,
      "learning_rate": 4.948000000000001e-05,
      "loss": 0.1959,
      "step": 40
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.599441409111023,
      "learning_rate": 4.9346666666666666e-05,
      "loss": 0.1506,
      "step": 50
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.3955787122249603,
      "learning_rate": 4.921333333333333e-05,
      "loss": 0.121,
      "step": 60
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.4029935896396637,
      "learning_rate": 4.9080000000000004e-05,
      "loss": 0.1248,
      "step": 70
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.548945963382721,
      "learning_rate": 4.894666666666667e-05,
      "loss": 0.1142,
      "step": 80
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.4278471767902374,
      "learning_rate": 4.8813333333333336e-05,
      "loss": 0.1225,
      "step": 90
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.39881154894828796,
      "learning_rate": 4.868e-05,
      "loss": 0.1254,
      "step": 100
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.369457483291626,
      "learning_rate": 4.854666666666667e-05,
      "loss": 0.098,
      "step": 110
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.6163970828056335,
      "learning_rate": 4.841333333333334e-05,
      "loss": 0.1535,
      "step": 120
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.35359740257263184,
      "learning_rate": 4.8280000000000005e-05,
      "loss": 0.0954,
      "step": 130
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.4719283878803253,
      "learning_rate": 4.814666666666667e-05,
      "loss": 0.1217,
      "step": 140
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.723195493221283,
      "learning_rate": 4.801333333333334e-05,
      "loss": 0.1393,
      "step": 150
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.5839716792106628,
      "learning_rate": 4.788e-05,
      "loss": 0.1222,
      "step": 160
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.47337818145751953,
      "learning_rate": 4.774666666666667e-05,
      "loss": 0.1122,
      "step": 170
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.5207918286323547,
      "learning_rate": 4.761333333333334e-05,
      "loss": 0.1054,
      "step": 180
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.4169235825538635,
      "learning_rate": 4.748e-05,
      "loss": 0.0996,
      "step": 190
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.8195927739143372,
      "learning_rate": 4.7346666666666665e-05,
      "loss": 0.0869,
      "step": 200
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.520354151725769,
      "learning_rate": 4.721333333333334e-05,
      "loss": 0.1095,
      "step": 210
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.44245192408561707,
      "learning_rate": 4.708e-05,
      "loss": 0.0931,
      "step": 220
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.7924697399139404,
      "learning_rate": 4.694666666666667e-05,
      "loss": 0.1295,
      "step": 230
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.3900239169597626,
      "learning_rate": 4.6813333333333335e-05,
      "loss": 0.0995,
      "step": 240
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.4457433223724365,
      "learning_rate": 4.668e-05,
      "loss": 0.1021,
      "step": 250
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.2983628809452057,
      "learning_rate": 4.6546666666666666e-05,
      "loss": 0.0903,
      "step": 260
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.6156854033470154,
      "learning_rate": 4.641333333333334e-05,
      "loss": 0.0734,
      "step": 270
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.5469792485237122,
      "learning_rate": 4.6280000000000004e-05,
      "loss": 0.1212,
      "step": 280
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.2666736841201782,
      "learning_rate": 4.614666666666667e-05,
      "loss": 0.084,
      "step": 290
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.9120122790336609,
      "learning_rate": 4.6013333333333336e-05,
      "loss": 0.0843,
      "step": 300
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.3726859986782074,
      "learning_rate": 4.588e-05,
      "loss": 0.072,
      "step": 310
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.8787695169448853,
      "learning_rate": 4.5746666666666674e-05,
      "loss": 0.0992,
      "step": 320
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.5943987965583801,
      "learning_rate": 4.561333333333333e-05,
      "loss": 0.08,
      "step": 330
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.5333069562911987,
      "learning_rate": 4.548e-05,
      "loss": 0.0625,
      "step": 340
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.4161955714225769,
      "learning_rate": 4.534666666666667e-05,
      "loss": 0.0859,
      "step": 350
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.4731924533843994,
      "learning_rate": 4.5213333333333336e-05,
      "loss": 0.078,
      "step": 360
    },
    {
      "epoch": 0.296,
      "grad_norm": 1.4746019840240479,
      "learning_rate": 4.508e-05,
      "loss": 0.1249,
      "step": 370
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.15772703289985657,
      "learning_rate": 4.494666666666667e-05,
      "loss": 0.0803,
      "step": 380
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.2764791250228882,
      "learning_rate": 4.4813333333333333e-05,
      "loss": 0.088,
      "step": 390
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.21676409244537354,
      "learning_rate": 4.468e-05,
      "loss": 0.0677,
      "step": 400
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.26012498140335083,
      "learning_rate": 4.454666666666667e-05,
      "loss": 0.0707,
      "step": 410
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.1610109657049179,
      "learning_rate": 4.441333333333334e-05,
      "loss": 0.0615,
      "step": 420
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.2391781359910965,
      "learning_rate": 4.428e-05,
      "loss": 0.0819,
      "step": 430
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.12543390691280365,
      "learning_rate": 4.414666666666667e-05,
      "loss": 0.0801,
      "step": 440
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.46015748381614685,
      "learning_rate": 4.4013333333333334e-05,
      "loss": 0.0961,
      "step": 450
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.19645625352859497,
      "learning_rate": 4.388000000000001e-05,
      "loss": 0.0572,
      "step": 460
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.45620831847190857,
      "learning_rate": 4.374666666666667e-05,
      "loss": 0.0505,
      "step": 470
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.8377721905708313,
      "learning_rate": 4.361333333333333e-05,
      "loss": 0.0619,
      "step": 480
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.4464917778968811,
      "learning_rate": 4.3480000000000004e-05,
      "loss": 0.0808,
      "step": 490
    },
    {
      "epoch": 0.4,
      "grad_norm": 2.8062376976013184,
      "learning_rate": 4.334666666666667e-05,
      "loss": 0.0813,
      "step": 500
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.3470776081085205,
      "learning_rate": 4.3213333333333335e-05,
      "loss": 0.0432,
      "step": 510
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.5910359025001526,
      "learning_rate": 4.308e-05,
      "loss": 0.0477,
      "step": 520
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.20518267154693604,
      "learning_rate": 4.2946666666666667e-05,
      "loss": 0.0715,
      "step": 530
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.4863033890724182,
      "learning_rate": 4.281333333333333e-05,
      "loss": 0.049,
      "step": 540
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.0983697846531868,
      "learning_rate": 4.2680000000000005e-05,
      "loss": 0.0429,
      "step": 550
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.37112411856651306,
      "learning_rate": 4.254666666666667e-05,
      "loss": 0.0713,
      "step": 560
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.48021748661994934,
      "learning_rate": 4.241333333333333e-05,
      "loss": 0.0575,
      "step": 570
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.5321270227432251,
      "learning_rate": 4.228e-05,
      "loss": 0.0663,
      "step": 580
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.09775352478027344,
      "learning_rate": 4.214666666666667e-05,
      "loss": 0.0355,
      "step": 590
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.22570322453975677,
      "learning_rate": 4.201333333333334e-05,
      "loss": 0.0683,
      "step": 600
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.6270821690559387,
      "learning_rate": 4.1880000000000006e-05,
      "loss": 0.0816,
      "step": 610
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.6508610248565674,
      "learning_rate": 4.1746666666666665e-05,
      "loss": 0.049,
      "step": 620
    },
    {
      "epoch": 0.504,
      "grad_norm": 1.6378908157348633,
      "learning_rate": 4.161333333333334e-05,
      "loss": 0.0694,
      "step": 630
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.1111549437046051,
      "learning_rate": 4.148e-05,
      "loss": 0.0551,
      "step": 640
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.5508831739425659,
      "learning_rate": 4.134666666666667e-05,
      "loss": 0.0602,
      "step": 650
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.10409526526927948,
      "learning_rate": 4.1213333333333334e-05,
      "loss": 0.0473,
      "step": 660
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.3494764268398285,
      "learning_rate": 4.108e-05,
      "loss": 0.0438,
      "step": 670
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.21448194980621338,
      "learning_rate": 4.0946666666666665e-05,
      "loss": 0.0496,
      "step": 680
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.8527529239654541,
      "learning_rate": 4.081333333333334e-05,
      "loss": 0.0316,
      "step": 690
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.07365932315587997,
      "learning_rate": 4.0680000000000004e-05,
      "loss": 0.0364,
      "step": 700
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.3278754651546478,
      "learning_rate": 4.054666666666667e-05,
      "loss": 0.0468,
      "step": 710
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.2589956223964691,
      "learning_rate": 4.0413333333333335e-05,
      "loss": 0.0578,
      "step": 720
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.27701857686042786,
      "learning_rate": 4.028e-05,
      "loss": 0.0322,
      "step": 730
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.333749383687973,
      "learning_rate": 4.014666666666667e-05,
      "loss": 0.047,
      "step": 740
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.0059343576431274,
      "learning_rate": 4.001333333333334e-05,
      "loss": 0.0541,
      "step": 750
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.9253187775611877,
      "learning_rate": 3.988e-05,
      "loss": 0.0423,
      "step": 760
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.683786928653717,
      "learning_rate": 3.974666666666667e-05,
      "loss": 0.0628,
      "step": 770
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.5697533488273621,
      "learning_rate": 3.9613333333333336e-05,
      "loss": 0.0314,
      "step": 780
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.14050067961215973,
      "learning_rate": 3.948e-05,
      "loss": 0.0423,
      "step": 790
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.30743032693862915,
      "learning_rate": 3.9346666666666674e-05,
      "loss": 0.0541,
      "step": 800
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.37896549701690674,
      "learning_rate": 3.921333333333333e-05,
      "loss": 0.0356,
      "step": 810
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.3513484299182892,
      "learning_rate": 3.908e-05,
      "loss": 0.0316,
      "step": 820
    },
    {
      "epoch": 0.664,
      "grad_norm": 1.3649225234985352,
      "learning_rate": 3.894666666666667e-05,
      "loss": 0.0407,
      "step": 830
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.11522281914949417,
      "learning_rate": 3.881333333333334e-05,
      "loss": 0.0283,
      "step": 840
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.1916142851114273,
      "learning_rate": 3.868e-05,
      "loss": 0.0364,
      "step": 850
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.20916549861431122,
      "learning_rate": 3.854666666666667e-05,
      "loss": 0.0429,
      "step": 860
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.09280890971422195,
      "learning_rate": 3.8413333333333334e-05,
      "loss": 0.025,
      "step": 870
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.055388424545526505,
      "learning_rate": 3.828e-05,
      "loss": 0.0313,
      "step": 880
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.7497703433036804,
      "learning_rate": 3.814666666666667e-05,
      "loss": 0.0394,
      "step": 890
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.1843084841966629,
      "learning_rate": 3.801333333333333e-05,
      "loss": 0.0364,
      "step": 900
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.16572028398513794,
      "learning_rate": 3.788e-05,
      "loss": 0.0115,
      "step": 910
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.2523990273475647,
      "learning_rate": 3.774666666666667e-05,
      "loss": 0.0419,
      "step": 920
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.341363787651062,
      "learning_rate": 3.7613333333333335e-05,
      "loss": 0.0575,
      "step": 930
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.08907736837863922,
      "learning_rate": 3.748000000000001e-05,
      "loss": 0.0293,
      "step": 940
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.18477687239646912,
      "learning_rate": 3.7346666666666666e-05,
      "loss": 0.0139,
      "step": 950
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.6335521340370178,
      "learning_rate": 3.721333333333333e-05,
      "loss": 0.0218,
      "step": 960
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.10771394520998001,
      "learning_rate": 3.7080000000000004e-05,
      "loss": 0.0257,
      "step": 970
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.10493570566177368,
      "learning_rate": 3.694666666666667e-05,
      "loss": 0.0652,
      "step": 980
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.6398709416389465,
      "learning_rate": 3.6813333333333335e-05,
      "loss": 0.0234,
      "step": 990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.22245824337005615,
      "learning_rate": 3.668e-05,
      "loss": 0.0424,
      "step": 1000
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.1053691953420639,
      "learning_rate": 3.654666666666667e-05,
      "loss": 0.0479,
      "step": 1010
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.10312707722187042,
      "learning_rate": 3.641333333333333e-05,
      "loss": 0.0256,
      "step": 1020
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.7924436330795288,
      "learning_rate": 3.6280000000000005e-05,
      "loss": 0.0209,
      "step": 1030
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.2167014330625534,
      "learning_rate": 3.614666666666667e-05,
      "loss": 0.0194,
      "step": 1040
    },
    {
      "epoch": 0.84,
      "grad_norm": 1.0310699939727783,
      "learning_rate": 3.6013333333333336e-05,
      "loss": 0.0449,
      "step": 1050
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.1204463541507721,
      "learning_rate": 3.588e-05,
      "loss": 0.0402,
      "step": 1060
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.6437186598777771,
      "learning_rate": 3.574666666666667e-05,
      "loss": 0.0373,
      "step": 1070
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.05883687734603882,
      "learning_rate": 3.561333333333334e-05,
      "loss": 0.0107,
      "step": 1080
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.056872185319662094,
      "learning_rate": 3.548e-05,
      "loss": 0.0263,
      "step": 1090
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.04876440390944481,
      "learning_rate": 3.5346666666666665e-05,
      "loss": 0.0182,
      "step": 1100
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.07354164868593216,
      "learning_rate": 3.521333333333334e-05,
      "loss": 0.0141,
      "step": 1110
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.04643619805574417,
      "learning_rate": 3.508e-05,
      "loss": 0.0481,
      "step": 1120
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.40935832262039185,
      "learning_rate": 3.494666666666667e-05,
      "loss": 0.0378,
      "step": 1130
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.08715739101171494,
      "learning_rate": 3.4813333333333334e-05,
      "loss": 0.0237,
      "step": 1140
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.23283518850803375,
      "learning_rate": 3.468e-05,
      "loss": 0.0114,
      "step": 1150
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.07038655877113342,
      "learning_rate": 3.4546666666666666e-05,
      "loss": 0.0195,
      "step": 1160
    },
    {
      "epoch": 0.936,
      "grad_norm": 1.9838405847549438,
      "learning_rate": 3.441333333333334e-05,
      "loss": 0.0127,
      "step": 1170
    },
    {
      "epoch": 0.944,
      "grad_norm": 1.0846611261367798,
      "learning_rate": 3.4280000000000004e-05,
      "loss": 0.0378,
      "step": 1180
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.0782032310962677,
      "learning_rate": 3.414666666666666e-05,
      "loss": 0.017,
      "step": 1190
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.9640313386917114,
      "learning_rate": 3.4013333333333335e-05,
      "loss": 0.0282,
      "step": 1200
    },
    {
      "epoch": 0.968,
      "grad_norm": 1.0370265245437622,
      "learning_rate": 3.388e-05,
      "loss": 0.0248,
      "step": 1210
    },
    {
      "epoch": 0.976,
      "grad_norm": 2.4072957038879395,
      "learning_rate": 3.374666666666667e-05,
      "loss": 0.0198,
      "step": 1220
    },
    {
      "epoch": 0.984,
      "grad_norm": 5.72052526473999,
      "learning_rate": 3.361333333333333e-05,
      "loss": 0.0111,
      "step": 1230
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.06121743842959404,
      "learning_rate": 3.348e-05,
      "loss": 0.0156,
      "step": 1240
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.024548199027776718,
      "learning_rate": 3.334666666666667e-05,
      "loss": 0.0089,
      "step": 1250
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.02670733444392681,
      "learning_rate": 3.3213333333333336e-05,
      "loss": 0.0133,
      "step": 1260
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.06667739897966385,
      "learning_rate": 3.308e-05,
      "loss": 0.0125,
      "step": 1270
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.06688468158245087,
      "learning_rate": 3.294666666666667e-05,
      "loss": 0.0107,
      "step": 1280
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.5688298940658569,
      "learning_rate": 3.281333333333333e-05,
      "loss": 0.0105,
      "step": 1290
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.02321636490523815,
      "learning_rate": 3.268e-05,
      "loss": 0.0148,
      "step": 1300
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.06200367957353592,
      "learning_rate": 3.254666666666667e-05,
      "loss": 0.0261,
      "step": 1310
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.01723422110080719,
      "learning_rate": 3.241333333333334e-05,
      "loss": 0.0238,
      "step": 1320
    },
    {
      "epoch": 1.064,
      "grad_norm": 2.836822509765625,
      "learning_rate": 3.2279999999999996e-05,
      "loss": 0.0116,
      "step": 1330
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.16669797897338867,
      "learning_rate": 3.214666666666667e-05,
      "loss": 0.0108,
      "step": 1340
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.09517192095518112,
      "learning_rate": 3.2013333333333334e-05,
      "loss": 0.0123,
      "step": 1350
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.16559594869613647,
      "learning_rate": 3.188e-05,
      "loss": 0.0067,
      "step": 1360
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.07994434982538223,
      "learning_rate": 3.174666666666667e-05,
      "loss": 0.0318,
      "step": 1370
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.023265160620212555,
      "learning_rate": 3.161333333333333e-05,
      "loss": 0.0045,
      "step": 1380
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.018916891887784004,
      "learning_rate": 3.1480000000000004e-05,
      "loss": 0.0111,
      "step": 1390
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.3286167085170746,
      "learning_rate": 3.134666666666667e-05,
      "loss": 0.0136,
      "step": 1400
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.5879097580909729,
      "learning_rate": 3.1213333333333335e-05,
      "loss": 0.0191,
      "step": 1410
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.06769593060016632,
      "learning_rate": 3.108e-05,
      "loss": 0.0064,
      "step": 1420
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.01744481548666954,
      "learning_rate": 3.0946666666666666e-05,
      "loss": 0.0172,
      "step": 1430
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.07074908167123795,
      "learning_rate": 3.081333333333333e-05,
      "loss": 0.0068,
      "step": 1440
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.6209830641746521,
      "learning_rate": 3.0680000000000004e-05,
      "loss": 0.0256,
      "step": 1450
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.038061343133449554,
      "learning_rate": 3.054666666666667e-05,
      "loss": 0.0161,
      "step": 1460
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.07573393732309341,
      "learning_rate": 3.0413333333333332e-05,
      "loss": 0.0289,
      "step": 1470
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.08475729823112488,
      "learning_rate": 3.028e-05,
      "loss": 0.0217,
      "step": 1480
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.03236937150359154,
      "learning_rate": 3.0146666666666667e-05,
      "loss": 0.0136,
      "step": 1490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.11587388068437576,
      "learning_rate": 3.0013333333333333e-05,
      "loss": 0.0242,
      "step": 1500
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.02265162020921707,
      "learning_rate": 2.9880000000000002e-05,
      "loss": 0.005,
      "step": 1510
    },
    {
      "epoch": 1.216,
      "grad_norm": 1.291396141052246,
      "learning_rate": 2.9746666666666668e-05,
      "loss": 0.0181,
      "step": 1520
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.0230921171605587,
      "learning_rate": 2.9613333333333337e-05,
      "loss": 0.0173,
      "step": 1530
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.04079794883728027,
      "learning_rate": 2.9480000000000002e-05,
      "loss": 0.0262,
      "step": 1540
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.15199130773544312,
      "learning_rate": 2.9346666666666668e-05,
      "loss": 0.0222,
      "step": 1550
    },
    {
      "epoch": 1.248,
      "grad_norm": 1.046776533126831,
      "learning_rate": 2.9213333333333337e-05,
      "loss": 0.018,
      "step": 1560
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.046363577246665955,
      "learning_rate": 2.9080000000000003e-05,
      "loss": 0.0062,
      "step": 1570
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.023862522095441818,
      "learning_rate": 2.8946666666666665e-05,
      "loss": 0.0037,
      "step": 1580
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.022019382566213608,
      "learning_rate": 2.8813333333333338e-05,
      "loss": 0.0149,
      "step": 1590
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.35982370376586914,
      "learning_rate": 2.868e-05,
      "loss": 0.005,
      "step": 1600
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.040120627731084824,
      "learning_rate": 2.8546666666666666e-05,
      "loss": 0.0065,
      "step": 1610
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.04999068006873131,
      "learning_rate": 2.8413333333333335e-05,
      "loss": 0.0113,
      "step": 1620
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.07578302174806595,
      "learning_rate": 2.828e-05,
      "loss": 0.0285,
      "step": 1630
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.41835007071495056,
      "learning_rate": 2.8146666666666666e-05,
      "loss": 0.0086,
      "step": 1640
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.031727153807878494,
      "learning_rate": 2.8013333333333335e-05,
      "loss": 0.0023,
      "step": 1650
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.08982598781585693,
      "learning_rate": 2.788e-05,
      "loss": 0.0057,
      "step": 1660
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.03623654320836067,
      "learning_rate": 2.7746666666666666e-05,
      "loss": 0.02,
      "step": 1670
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.051856786012649536,
      "learning_rate": 2.7613333333333335e-05,
      "loss": 0.0114,
      "step": 1680
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.053606051951646805,
      "learning_rate": 2.748e-05,
      "loss": 0.0099,
      "step": 1690
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.02811809442937374,
      "learning_rate": 2.734666666666667e-05,
      "loss": 0.0032,
      "step": 1700
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.04407722130417824,
      "learning_rate": 2.7213333333333336e-05,
      "loss": 0.0042,
      "step": 1710
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.07565181702375412,
      "learning_rate": 2.7079999999999998e-05,
      "loss": 0.0025,
      "step": 1720
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.041865088045597076,
      "learning_rate": 2.694666666666667e-05,
      "loss": 0.0279,
      "step": 1730
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.017550028860569,
      "learning_rate": 2.6813333333333336e-05,
      "loss": 0.0047,
      "step": 1740
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.04975390434265137,
      "learning_rate": 2.668e-05,
      "loss": 0.0162,
      "step": 1750
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.020254971459507942,
      "learning_rate": 2.654666666666667e-05,
      "loss": 0.0234,
      "step": 1760
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.17358587682247162,
      "learning_rate": 2.6413333333333333e-05,
      "loss": 0.0061,
      "step": 1770
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.6214284896850586,
      "learning_rate": 2.628e-05,
      "loss": 0.0263,
      "step": 1780
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.03937997296452522,
      "learning_rate": 2.6146666666666668e-05,
      "loss": 0.0042,
      "step": 1790
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.4345841109752655,
      "learning_rate": 2.6013333333333334e-05,
      "loss": 0.0062,
      "step": 1800
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.02218162640929222,
      "learning_rate": 2.588e-05,
      "loss": 0.0034,
      "step": 1810
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.020569507032632828,
      "learning_rate": 2.574666666666667e-05,
      "loss": 0.0023,
      "step": 1820
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.07576340436935425,
      "learning_rate": 2.5613333333333334e-05,
      "loss": 0.0037,
      "step": 1830
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.0208080243319273,
      "learning_rate": 2.5480000000000003e-05,
      "loss": 0.0038,
      "step": 1840
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.03569331765174866,
      "learning_rate": 2.534666666666667e-05,
      "loss": 0.0026,
      "step": 1850
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.021976549178361893,
      "learning_rate": 2.5213333333333335e-05,
      "loss": 0.0244,
      "step": 1860
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.24089941382408142,
      "learning_rate": 2.5080000000000004e-05,
      "loss": 0.028,
      "step": 1870
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.050919871777296066,
      "learning_rate": 2.494666666666667e-05,
      "loss": 0.0041,
      "step": 1880
    },
    {
      "epoch": 1.512,
      "grad_norm": 1.6715290546417236,
      "learning_rate": 2.4813333333333335e-05,
      "loss": 0.0073,
      "step": 1890
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.03323878347873688,
      "learning_rate": 2.468e-05,
      "loss": 0.0164,
      "step": 1900
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.023191623389720917,
      "learning_rate": 2.4546666666666667e-05,
      "loss": 0.003,
      "step": 1910
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.034835539758205414,
      "learning_rate": 2.4413333333333336e-05,
      "loss": 0.0059,
      "step": 1920
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.043912939727306366,
      "learning_rate": 2.428e-05,
      "loss": 0.0038,
      "step": 1930
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.0457770936191082,
      "learning_rate": 2.4146666666666667e-05,
      "loss": 0.0026,
      "step": 1940
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.659515917301178,
      "learning_rate": 2.4013333333333336e-05,
      "loss": 0.0141,
      "step": 1950
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.04769846796989441,
      "learning_rate": 2.3880000000000002e-05,
      "loss": 0.0025,
      "step": 1960
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.018178464844822884,
      "learning_rate": 2.3746666666666667e-05,
      "loss": 0.0158,
      "step": 1970
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.048153895884752274,
      "learning_rate": 2.3613333333333333e-05,
      "loss": 0.0117,
      "step": 1980
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.030374696478247643,
      "learning_rate": 2.3480000000000002e-05,
      "loss": 0.0275,
      "step": 1990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.03962216153740883,
      "learning_rate": 2.3346666666666668e-05,
      "loss": 0.026,
      "step": 2000
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.038019850850105286,
      "learning_rate": 2.3213333333333334e-05,
      "loss": 0.0196,
      "step": 2010
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.14226199686527252,
      "learning_rate": 2.3080000000000003e-05,
      "loss": 0.0033,
      "step": 2020
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.05468031018972397,
      "learning_rate": 2.294666666666667e-05,
      "loss": 0.0039,
      "step": 2030
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.04376549273729324,
      "learning_rate": 2.2813333333333334e-05,
      "loss": 0.0161,
      "step": 2040
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.017710931599140167,
      "learning_rate": 2.268e-05,
      "loss": 0.0023,
      "step": 2050
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.016143865883350372,
      "learning_rate": 2.254666666666667e-05,
      "loss": 0.0112,
      "step": 2060
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.08071467280387878,
      "learning_rate": 2.2413333333333334e-05,
      "loss": 0.0035,
      "step": 2070
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.043730735778808594,
      "learning_rate": 2.228e-05,
      "loss": 0.003,
      "step": 2080
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.037524785846471786,
      "learning_rate": 2.214666666666667e-05,
      "loss": 0.0162,
      "step": 2090
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.02835751511156559,
      "learning_rate": 2.201333333333333e-05,
      "loss": 0.0034,
      "step": 2100
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.021290555596351624,
      "learning_rate": 2.188e-05,
      "loss": 0.012,
      "step": 2110
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.06980147957801819,
      "learning_rate": 2.174666666666667e-05,
      "loss": 0.021,
      "step": 2120
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.19311262667179108,
      "learning_rate": 2.1613333333333335e-05,
      "loss": 0.0038,
      "step": 2130
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.20755961537361145,
      "learning_rate": 2.148e-05,
      "loss": 0.0018,
      "step": 2140
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.021885154768824577,
      "learning_rate": 2.1346666666666667e-05,
      "loss": 0.0018,
      "step": 2150
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.061294253915548325,
      "learning_rate": 2.1213333333333336e-05,
      "loss": 0.0183,
      "step": 2160
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.028560766950249672,
      "learning_rate": 2.1079999999999998e-05,
      "loss": 0.0024,
      "step": 2170
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.02175820805132389,
      "learning_rate": 2.0946666666666667e-05,
      "loss": 0.0111,
      "step": 2180
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.08823073655366898,
      "learning_rate": 2.0813333333333336e-05,
      "loss": 0.0029,
      "step": 2190
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.08133137226104736,
      "learning_rate": 2.0680000000000002e-05,
      "loss": 0.0068,
      "step": 2200
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.01619219407439232,
      "learning_rate": 2.0546666666666668e-05,
      "loss": 0.0056,
      "step": 2210
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.021077973768115044,
      "learning_rate": 2.0413333333333333e-05,
      "loss": 0.0019,
      "step": 2220
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.01620861142873764,
      "learning_rate": 2.0280000000000002e-05,
      "loss": 0.0027,
      "step": 2230
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.7671815752983093,
      "learning_rate": 2.0146666666666668e-05,
      "loss": 0.0292,
      "step": 2240
    },
    {
      "epoch": 1.8,
      "grad_norm": 2.6970784664154053,
      "learning_rate": 2.0013333333333334e-05,
      "loss": 0.0097,
      "step": 2250
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.02390342205762863,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 0.007,
      "step": 2260
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.02106984332203865,
      "learning_rate": 1.974666666666667e-05,
      "loss": 0.0027,
      "step": 2270
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 1.0027225017547607,
      "learning_rate": 1.9613333333333334e-05,
      "loss": 0.0162,
      "step": 2280
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.015232925303280354,
      "learning_rate": 1.948e-05,
      "loss": 0.0021,
      "step": 2290
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.02187148667871952,
      "learning_rate": 1.934666666666667e-05,
      "loss": 0.0071,
      "step": 2300
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.03416157141327858,
      "learning_rate": 1.9213333333333335e-05,
      "loss": 0.0153,
      "step": 2310
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.023765452206134796,
      "learning_rate": 1.908e-05,
      "loss": 0.0021,
      "step": 2320
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.022863058373332024,
      "learning_rate": 1.894666666666667e-05,
      "loss": 0.0079,
      "step": 2330
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.018273083493113518,
      "learning_rate": 1.8813333333333335e-05,
      "loss": 0.002,
      "step": 2340
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.01749989204108715,
      "learning_rate": 1.868e-05,
      "loss": 0.0164,
      "step": 2350
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.0329606756567955,
      "learning_rate": 1.8546666666666666e-05,
      "loss": 0.0033,
      "step": 2360
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.024586718529462814,
      "learning_rate": 1.8413333333333335e-05,
      "loss": 0.0264,
      "step": 2370
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.026414327323436737,
      "learning_rate": 1.828e-05,
      "loss": 0.0126,
      "step": 2380
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.031876467168331146,
      "learning_rate": 1.8146666666666667e-05,
      "loss": 0.0034,
      "step": 2390
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.027662362903356552,
      "learning_rate": 1.8013333333333336e-05,
      "loss": 0.0062,
      "step": 2400
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.887274980545044,
      "learning_rate": 1.7879999999999998e-05,
      "loss": 0.0123,
      "step": 2410
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.05953998491168022,
      "learning_rate": 1.7746666666666667e-05,
      "loss": 0.0331,
      "step": 2420
    },
    {
      "epoch": 1.944,
      "grad_norm": 1.113466739654541,
      "learning_rate": 1.7613333333333333e-05,
      "loss": 0.0219,
      "step": 2430
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.056101564317941666,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 0.0044,
      "step": 2440
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.03798235207796097,
      "learning_rate": 1.7346666666666668e-05,
      "loss": 0.0033,
      "step": 2450
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.4785526990890503,
      "learning_rate": 1.7213333333333333e-05,
      "loss": 0.0198,
      "step": 2460
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.023621171712875366,
      "learning_rate": 1.7080000000000002e-05,
      "loss": 0.0117,
      "step": 2470
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.08291951566934586,
      "learning_rate": 1.6946666666666665e-05,
      "loss": 0.0189,
      "step": 2480
    },
    {
      "epoch": 1.992,
      "grad_norm": 4.342144966125488,
      "learning_rate": 1.6813333333333334e-05,
      "loss": 0.0095,
      "step": 2490
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.020068218931555748,
      "learning_rate": 1.668e-05,
      "loss": 0.0026,
      "step": 2500
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.032864220440387726,
      "learning_rate": 1.654666666666667e-05,
      "loss": 0.002,
      "step": 2510
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.11861108243465424,
      "learning_rate": 1.6413333333333334e-05,
      "loss": 0.0021,
      "step": 2520
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.03048337996006012,
      "learning_rate": 1.628e-05,
      "loss": 0.0066,
      "step": 2530
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.020986657589673996,
      "learning_rate": 1.614666666666667e-05,
      "loss": 0.0038,
      "step": 2540
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.022671673446893692,
      "learning_rate": 1.601333333333333e-05,
      "loss": 0.0174,
      "step": 2550
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.0229940302670002,
      "learning_rate": 1.588e-05,
      "loss": 0.0018,
      "step": 2560
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.0411827526986599,
      "learning_rate": 1.574666666666667e-05,
      "loss": 0.0112,
      "step": 2570
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.03313854709267616,
      "learning_rate": 1.5613333333333335e-05,
      "loss": 0.0285,
      "step": 2580
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.06525207310914993,
      "learning_rate": 1.548e-05,
      "loss": 0.0139,
      "step": 2590
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.058268796652555466,
      "learning_rate": 1.5346666666666667e-05,
      "loss": 0.0026,
      "step": 2600
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.017327649518847466,
      "learning_rate": 1.5213333333333336e-05,
      "loss": 0.0025,
      "step": 2610
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.042703043669462204,
      "learning_rate": 1.508e-05,
      "loss": 0.0034,
      "step": 2620
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.04656806215643883,
      "learning_rate": 1.4946666666666667e-05,
      "loss": 0.0032,
      "step": 2630
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.012427374720573425,
      "learning_rate": 1.4813333333333334e-05,
      "loss": 0.0018,
      "step": 2640
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.015187651850283146,
      "learning_rate": 1.4680000000000002e-05,
      "loss": 0.0056,
      "step": 2650
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.013564211316406727,
      "learning_rate": 1.4546666666666667e-05,
      "loss": 0.0015,
      "step": 2660
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.04553442448377609,
      "learning_rate": 1.4413333333333335e-05,
      "loss": 0.0136,
      "step": 2670
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.017502181231975555,
      "learning_rate": 1.4280000000000002e-05,
      "loss": 0.0178,
      "step": 2680
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.0228414386510849,
      "learning_rate": 1.4146666666666666e-05,
      "loss": 0.002,
      "step": 2690
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.0253713671118021,
      "learning_rate": 1.4013333333333334e-05,
      "loss": 0.0016,
      "step": 2700
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.0228599701076746,
      "learning_rate": 1.3880000000000001e-05,
      "loss": 0.0021,
      "step": 2710
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.013258998282253742,
      "learning_rate": 1.3746666666666667e-05,
      "loss": 0.0029,
      "step": 2720
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.024917196482419968,
      "learning_rate": 1.3613333333333334e-05,
      "loss": 0.0013,
      "step": 2730
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.0403783842921257,
      "learning_rate": 1.3480000000000001e-05,
      "loss": 0.0063,
      "step": 2740
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.030834490433335304,
      "learning_rate": 1.3346666666666669e-05,
      "loss": 0.0374,
      "step": 2750
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.016873426735401154,
      "learning_rate": 1.3213333333333333e-05,
      "loss": 0.0159,
      "step": 2760
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.01897509954869747,
      "learning_rate": 1.308e-05,
      "loss": 0.0096,
      "step": 2770
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.018081270158290863,
      "learning_rate": 1.2946666666666668e-05,
      "loss": 0.0017,
      "step": 2780
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.022070515900850296,
      "learning_rate": 1.2813333333333333e-05,
      "loss": 0.0142,
      "step": 2790
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.022353552281856537,
      "learning_rate": 1.268e-05,
      "loss": 0.0073,
      "step": 2800
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.021297825500369072,
      "learning_rate": 1.2546666666666668e-05,
      "loss": 0.0018,
      "step": 2810
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.02890409156680107,
      "learning_rate": 1.2413333333333334e-05,
      "loss": 0.0016,
      "step": 2820
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.01279112696647644,
      "learning_rate": 1.2280000000000001e-05,
      "loss": 0.0057,
      "step": 2830
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.029142389073967934,
      "learning_rate": 1.2146666666666667e-05,
      "loss": 0.0013,
      "step": 2840
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.016793226823210716,
      "learning_rate": 1.2013333333333334e-05,
      "loss": 0.0126,
      "step": 2850
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.019732598215341568,
      "learning_rate": 1.1880000000000001e-05,
      "loss": 0.015,
      "step": 2860
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.19836463034152985,
      "learning_rate": 1.1746666666666667e-05,
      "loss": 0.0037,
      "step": 2870
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.07590851187705994,
      "learning_rate": 1.1613333333333335e-05,
      "loss": 0.0154,
      "step": 2880
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.27755045890808105,
      "learning_rate": 1.148e-05,
      "loss": 0.0234,
      "step": 2890
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.026872221380472183,
      "learning_rate": 1.1346666666666666e-05,
      "loss": 0.0055,
      "step": 2900
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.02419973537325859,
      "learning_rate": 1.1213333333333333e-05,
      "loss": 0.0021,
      "step": 2910
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.017714139074087143,
      "learning_rate": 1.108e-05,
      "loss": 0.002,
      "step": 2920
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.011361518874764442,
      "learning_rate": 1.0946666666666668e-05,
      "loss": 0.0012,
      "step": 2930
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.018949396908283234,
      "learning_rate": 1.0813333333333334e-05,
      "loss": 0.0062,
      "step": 2940
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.020780690014362335,
      "learning_rate": 1.0680000000000001e-05,
      "loss": 0.0158,
      "step": 2950
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.103690966963768,
      "learning_rate": 1.0546666666666667e-05,
      "loss": 0.0097,
      "step": 2960
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.048090610653162,
      "learning_rate": 1.0413333333333332e-05,
      "loss": 0.0017,
      "step": 2970
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.01363697461783886,
      "learning_rate": 1.0280000000000002e-05,
      "loss": 0.0015,
      "step": 2980
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.02550058625638485,
      "learning_rate": 1.0146666666666667e-05,
      "loss": 0.0126,
      "step": 2990
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.016820119693875313,
      "learning_rate": 1.0013333333333335e-05,
      "loss": 0.0133,
      "step": 3000
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.02542950212955475,
      "learning_rate": 9.88e-06,
      "loss": 0.0014,
      "step": 3010
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.014088837429881096,
      "learning_rate": 9.746666666666666e-06,
      "loss": 0.0045,
      "step": 3020
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.01794377714395523,
      "learning_rate": 9.613333333333333e-06,
      "loss": 0.009,
      "step": 3030
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.021315181627869606,
      "learning_rate": 9.48e-06,
      "loss": 0.0156,
      "step": 3040
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.06114107742905617,
      "learning_rate": 9.346666666666668e-06,
      "loss": 0.0142,
      "step": 3050
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.04768122732639313,
      "learning_rate": 9.213333333333334e-06,
      "loss": 0.0034,
      "step": 3060
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.024780990555882454,
      "learning_rate": 9.080000000000001e-06,
      "loss": 0.0114,
      "step": 3070
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.9385781288146973,
      "learning_rate": 8.946666666666667e-06,
      "loss": 0.0057,
      "step": 3080
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.028978729620575905,
      "learning_rate": 8.813333333333333e-06,
      "loss": 0.0018,
      "step": 3090
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.06989987939596176,
      "learning_rate": 8.68e-06,
      "loss": 0.0019,
      "step": 3100
    },
    {
      "epoch": 2.488,
      "grad_norm": 1.7485023736953735,
      "learning_rate": 8.546666666666667e-06,
      "loss": 0.0101,
      "step": 3110
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.009467016905546188,
      "learning_rate": 8.413333333333335e-06,
      "loss": 0.0137,
      "step": 3120
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.017411233857274055,
      "learning_rate": 8.28e-06,
      "loss": 0.0109,
      "step": 3130
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.013299965299665928,
      "learning_rate": 8.146666666666668e-06,
      "loss": 0.0016,
      "step": 3140
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.02164270170032978,
      "learning_rate": 8.013333333333333e-06,
      "loss": 0.0017,
      "step": 3150
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.03913513198494911,
      "learning_rate": 7.879999999999999e-06,
      "loss": 0.0019,
      "step": 3160
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.021217700093984604,
      "learning_rate": 7.746666666666668e-06,
      "loss": 0.0014,
      "step": 3170
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.014421963132917881,
      "learning_rate": 7.613333333333334e-06,
      "loss": 0.0013,
      "step": 3180
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.021407639607787132,
      "learning_rate": 7.480000000000001e-06,
      "loss": 0.003,
      "step": 3190
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.0105436434969306,
      "learning_rate": 7.346666666666667e-06,
      "loss": 0.0046,
      "step": 3200
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.01693335734307766,
      "learning_rate": 7.2133333333333334e-06,
      "loss": 0.0145,
      "step": 3210
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.02491658926010132,
      "learning_rate": 7.080000000000001e-06,
      "loss": 0.0012,
      "step": 3220
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.015982652083039284,
      "learning_rate": 6.9466666666666665e-06,
      "loss": 0.0017,
      "step": 3230
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.021803464740514755,
      "learning_rate": 6.813333333333334e-06,
      "loss": 0.0146,
      "step": 3240
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.034069932997226715,
      "learning_rate": 6.68e-06,
      "loss": 0.0016,
      "step": 3250
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.026343047618865967,
      "learning_rate": 6.546666666666668e-06,
      "loss": 0.0014,
      "step": 3260
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.06520086526870728,
      "learning_rate": 6.4133333333333335e-06,
      "loss": 0.01,
      "step": 3270
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.010276010259985924,
      "learning_rate": 6.28e-06,
      "loss": 0.0014,
      "step": 3280
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.013096533715724945,
      "learning_rate": 6.146666666666667e-06,
      "loss": 0.0111,
      "step": 3290
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.01666710339486599,
      "learning_rate": 6.013333333333333e-06,
      "loss": 0.0019,
      "step": 3300
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.02849818393588066,
      "learning_rate": 5.8800000000000005e-06,
      "loss": 0.0014,
      "step": 3310
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.042307838797569275,
      "learning_rate": 5.746666666666667e-06,
      "loss": 0.0014,
      "step": 3320
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.014484504237771034,
      "learning_rate": 5.6133333333333335e-06,
      "loss": 0.0019,
      "step": 3330
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.021851493045687675,
      "learning_rate": 5.48e-06,
      "loss": 0.0206,
      "step": 3340
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.020787153393030167,
      "learning_rate": 5.3466666666666674e-06,
      "loss": 0.0017,
      "step": 3350
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.010251540690660477,
      "learning_rate": 5.213333333333333e-06,
      "loss": 0.0255,
      "step": 3360
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.01815343089401722,
      "learning_rate": 5.08e-06,
      "loss": 0.0077,
      "step": 3370
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.01483586709946394,
      "learning_rate": 4.946666666666667e-06,
      "loss": 0.002,
      "step": 3380
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.01713448204100132,
      "learning_rate": 4.8133333333333336e-06,
      "loss": 0.0014,
      "step": 3390
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.01630137488245964,
      "learning_rate": 4.68e-06,
      "loss": 0.0014,
      "step": 3400
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.05967416614294052,
      "learning_rate": 4.5466666666666675e-06,
      "loss": 0.0137,
      "step": 3410
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.021267181262373924,
      "learning_rate": 4.413333333333333e-06,
      "loss": 0.0015,
      "step": 3420
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.014818805269896984,
      "learning_rate": 4.28e-06,
      "loss": 0.026,
      "step": 3430
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.028840111568570137,
      "learning_rate": 4.146666666666667e-06,
      "loss": 0.0016,
      "step": 3440
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.020551428198814392,
      "learning_rate": 4.013333333333334e-06,
      "loss": 0.0095,
      "step": 3450
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.042357705533504486,
      "learning_rate": 3.88e-06,
      "loss": 0.0022,
      "step": 3460
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.015563207678496838,
      "learning_rate": 3.746666666666667e-06,
      "loss": 0.0017,
      "step": 3470
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.0360402874648571,
      "learning_rate": 3.613333333333334e-06,
      "loss": 0.0022,
      "step": 3480
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.8297391533851624,
      "learning_rate": 3.4799999999999997e-06,
      "loss": 0.0076,
      "step": 3490
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.036785680800676346,
      "learning_rate": 3.3466666666666667e-06,
      "loss": 0.0019,
      "step": 3500
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.014069750905036926,
      "learning_rate": 3.2133333333333336e-06,
      "loss": 0.0016,
      "step": 3510
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.01434671226888895,
      "learning_rate": 3.08e-06,
      "loss": 0.0025,
      "step": 3520
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.025149568915367126,
      "learning_rate": 2.9466666666666667e-06,
      "loss": 0.008,
      "step": 3530
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.03654412925243378,
      "learning_rate": 2.8133333333333336e-06,
      "loss": 0.0025,
      "step": 3540
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.17830303311347961,
      "learning_rate": 2.68e-06,
      "loss": 0.0023,
      "step": 3550
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.03836676478385925,
      "learning_rate": 2.5466666666666667e-06,
      "loss": 0.0021,
      "step": 3560
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.053317490965127945,
      "learning_rate": 2.4133333333333332e-06,
      "loss": 0.0053,
      "step": 3570
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.0174739770591259,
      "learning_rate": 2.28e-06,
      "loss": 0.0017,
      "step": 3580
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.01576637662947178,
      "learning_rate": 2.1466666666666667e-06,
      "loss": 0.0064,
      "step": 3590
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.04467381164431572,
      "learning_rate": 2.0133333333333333e-06,
      "loss": 0.0016,
      "step": 3600
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.012722719460725784,
      "learning_rate": 1.8800000000000002e-06,
      "loss": 0.0011,
      "step": 3610
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.01698211394250393,
      "learning_rate": 1.7466666666666665e-06,
      "loss": 0.0019,
      "step": 3620
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.01591440476477146,
      "learning_rate": 1.6133333333333333e-06,
      "loss": 0.0013,
      "step": 3630
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.02073007822036743,
      "learning_rate": 1.4800000000000002e-06,
      "loss": 0.0088,
      "step": 3640
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.03258272632956505,
      "learning_rate": 1.3466666666666668e-06,
      "loss": 0.0034,
      "step": 3650
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.011748281307518482,
      "learning_rate": 1.2133333333333333e-06,
      "loss": 0.0014,
      "step": 3660
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.01205145288258791,
      "learning_rate": 1.08e-06,
      "loss": 0.0014,
      "step": 3670
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.026148566976189613,
      "learning_rate": 9.466666666666667e-07,
      "loss": 0.0013,
      "step": 3680
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.010426531545817852,
      "learning_rate": 8.133333333333333e-07,
      "loss": 0.0219,
      "step": 3690
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.01635640114545822,
      "learning_rate": 6.8e-07,
      "loss": 0.0036,
      "step": 3700
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.016542550176382065,
      "learning_rate": 5.466666666666667e-07,
      "loss": 0.0017,
      "step": 3710
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.012890750542283058,
      "learning_rate": 4.133333333333334e-07,
      "loss": 0.0015,
      "step": 3720
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.02910511940717697,
      "learning_rate": 2.8e-07,
      "loss": 0.0011,
      "step": 3730
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.010645456612110138,
      "learning_rate": 1.4666666666666668e-07,
      "loss": 0.0067,
      "step": 3740
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.0157926045358181,
      "learning_rate": 1.3333333333333335e-08,
      "loss": 0.0016,
      "step": 3750
    }
  ],
  "logging_steps": 10,
  "max_steps": 3750,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 496850192640000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
